{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10cd6597-ac8a-4f42-8500-e9b1c8f47d29",
   "metadata": {},
   "source": [
    "# Resume Semantic Search Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab56fb3-9189-4b07-a97e-ebb442c17ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a811db1-f557-4222-853a-8049237c15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d8cbd3-1d5d-4da3-bd3e-76683055bb23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Installing required packages \n",
    "\n",
    "pip install pandas faiss-cpu streamlit sentence-transformers requests pdfplumber tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae243534-52a8-4d6e-843d-4146c896fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pdfplumber\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ed457-4345-4289-abce-86a6c299f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download file from Google Drive using file ID\n",
    "\n",
    "def download_file_from_google_drive(file_id, destination):\n",
    "    URL = \"https://drive.google.com/drive/folders/1A0GPGrH7rLlAFNJ-RftYzLp28fJ0kKSU\"  # URL of the Google Drive folder\n",
    "    session = requests.Session()  # Initializing a session to maintain connection\n",
    "    response = session.get(URL, params={'id': file_id}, stream=True) # Send request to get file with specific file_id\n",
    "    token = get_confirm_token(response) # Check for token to confirm download\n",
    "    \n",
    "    if token:\n",
    "        # If token found, send request again with confirmation token\n",
    "        params = {'id': file_id, 'confirm': token}\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "    # Save content of response to destination file\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "# Function to extract confirmation token from response cookies\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "# Function to save response content to a file\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "    # Write chunks of content to destination file\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "# Function to load resumes from Google Drive links\n",
    "def load_resumes_from_links(links):\n",
    "    resumes = []\n",
    "    for link in links:\n",
    "        # Extract file_id from Google Drive link\n",
    "        file_id = link.split('/')[-2]\n",
    "        # Define destination file path as file_id.pdf\n",
    "        destination = f\"{file_id}.pdf\"\n",
    "\n",
    "        # Download file from Google Drive and append file path to resumes list\n",
    "        download_file_from_google_drive(file_id, destination)\n",
    "        resumes.append(destination)\n",
    "    return resumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32827711-20d9-4ad1-b762-c84590d6a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDFs using pdfplumber\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        # Open PDF file with pdfplumber\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            text = ''\n",
    "             # Iterate through each page in the PDF and extract text\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or ''\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        # Handle exceptions if PDF cannot be read\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to embed resumes into vectors using SentenceTransformer\n",
    "def embed_resumes(resumes):\n",
    "    # Initializing SentenceTransformer model for embedding resumes\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = []\n",
    "    # Iterate through each resume file path\n",
    "    for resume in resumes:\n",
    "        # Extract text from PDF resume\n",
    "        text = extract_text_from_pdf(resume)\n",
    "        # If text extraction successful, encode text into embedding vector\n",
    "        if text:  # Proceed only if text extraction was successful\n",
    "            embedding = model.encode(text)\n",
    "            embeddings.append(embedding)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c836415-939c-4ad3-921f-33e23a51c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build FAISS index for embeddings\n",
    "def build_faiss_index(embeddings):\n",
    "    # Determine dimensionality of embeddings\n",
    "    d = len(embeddings[0])\n",
    "    # Initialize FAISS index with L2 distance metric\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    # Add embeddings to FAISS index\n",
    "    index.add(np.array(embeddings))\n",
    "    \n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a4f11-d924-4b81-9f4a-9d23af9dd373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search resumes using FAISS index and return results\n",
    "def search_resumes(query, index, resumes):\n",
    "    # Initialize SentenceTransformer model for embedding queries\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    # Encode query into embedding vector\n",
    "    query_embedding = model.encode(query)\n",
    "    # Search FAISS index for nearest neighbors to query embedding\n",
    "    D, I = index.search(np.array([query_embedding]), k=10)\n",
    "    # Return resumes corresponding to nearest neighbor indices\n",
    "    return [resumes[i] for i in I[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc4c63-5a11-403d-94b7-e5ebfb3025ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display search results in console\n",
    "def display_results_console(query, results):\n",
    "    print(f\"Query: {query}\")\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf65320-bce5-498a-b149-f1fedf3dc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display search results in Streamlit interface\n",
    "def main():\n",
    "    # Set title for Streamlit web application\n",
    "    st.title(\"Resume Semantic Search\")\n",
    "    \n",
    "    # Create text input box for user to enter search query\n",
    "    query = st.text_input(\"Enter your query:\")\n",
    "    \n",
    "    # Create button for user to trigger search\n",
    "    if st.button(\"Search\"):\n",
    "        # Define example Google Drive link(s) containing resumes\n",
    "        links = [\"https://drive.google.com/drive/folders/1A0GPGrH7rLlAFNJ-RftYzLp28fJ0kKSU\"]\n",
    "        \n",
    "        # Load resumes from Google Drive links\n",
    "        resumes = load_resumes_from_links(links)\n",
    "        \n",
    "        # Embed resumes into vector representations\n",
    "        embeddings = embed_resumes(resumes)\n",
    "        \n",
    "        # If embeddings successfully created, build FAISS index\n",
    "        if embeddings:\n",
    "            index = build_faiss_index(embeddings)\n",
    "            \n",
    "            # Perform semantic search based on user query\n",
    "            results = search_resumes(query, index, resumes)\n",
    "            \n",
    "            # Display search results in Streamlit interface\n",
    "            for result in results:\n",
    "                st.write(result)\n",
    "                \n",
    "            # Display search results in console\n",
    "            display_results_console(query, results)\n",
    "        else:\n",
    "            st.write(\"No valid resumes found or embeddings were not created.\")\n",
    "\n",
    "\n",
    "# Example usage in Streamlit application\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab94604-a78f-4a72-8c32-21b4dcdb753c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635461e2-dc39-4369-906e-2d126fae0196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
