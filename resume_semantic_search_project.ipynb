{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89501b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab56fb3-9189-4b07-a97e-ebb442c17ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch==2.1.1 in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (2.1.1+cu118)\n",
      "Requirement already satisfied: torchvision==0.16.1 in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (0.16.1+cu118)\n",
      "Requirement already satisfied: torchaudio==2.1.1 in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (2.1.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from torch==2.1.1) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from torch==2.1.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from torch==2.1.1) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from torch==2.1.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from torch==2.1.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from torch==2.1.1) (2024.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from torchvision==0.16.1) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from torchvision==0.16.1) (2.32.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from torchvision==0.16.1) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from jinja2->torch==2.1.1) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->torchvision==0.16.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->torchvision==0.16.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->torchvision==0.16.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->torchvision==0.16.1) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\rahul\\anaconda3\\envs\\llm\\lib\\site-packages (from sympy->torch==2.1.1) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a811db1-f557-4222-853a-8049237c15d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73fb163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tbb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95efbd0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/39/e1/08681583569f435347ced0535b27c073fcc9a927d9b4293c963092f2d01c/spacy-3.7.5-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading spacy-3.7.5-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Obtaining dependency information for spacy-legacy<3.1.0,>=3.0.11 from https://files.pythonhosted.org/packages/c3/55/12e842c70ff8828e34e543a2c7176dac4da006ca6901c9e8b43efab8bc6b/spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/71/46/af01a20ec368bd9cb49a1d2df15e3eca113bbf6952cc1f2a47f1c6801a7f/murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/c1/c3/dd044e6f62a3d317c461f6f0c153c6573ed13025752d779e514000c15dd2/cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/e4/fc/78cdbdb79f5d6d45949e72c32445d6c060977ad50a1dcfc0392622165f7c/preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.3.0,>=8.2.2 from https://files.pythonhosted.org/packages/5e/0e/5e7b24e046e0725eafc37ded0cd9bfaf789efb894101a7aca8a73dba81de/thinc-8.2.5-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/06/7c/34330a89da55610daa5f245ddce5aab81244321101614751e7537f125133/wasabi-1.1.3-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/eb/f5/e3f29993f673d91623df6413ba64e815dd2676fd7932cbc5e7347402ddae/srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.5.0,>=0.1.0 from https://files.pythonhosted.org/packages/2a/87/abd57374044e1f627f0a905ac33c1a7daab35a3a815abfea4e1bafd3fdb1/weasel-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (0.10.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Obtaining dependency information for langcodes<4.0.0,>=3.2.0 from https://files.pythonhosted.org/packages/58/70/4058ab0ebb082b18d06888e711baed7f33354a5e0b363bb627586d8c323a/langcodes-3.4.0-py3-none-any.whl.metadata\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Obtaining dependency information for language-data>=1.2 from https://files.pythonhosted.org/packages/12/5f/139464da89c49afcc8bb97ebad48818a535220ce01b1f24c61fb80dbe4d0/language_data-1.2.0-py3-none-any.whl.metadata\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/2f/09/da0592c74560cc33396504698122f7a56747c82a5e072ca7d2c3397898e1/blis-0.7.11-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/0c/00/3106b1854b45bd0474ced037dfe6b73b90fe68a68968cef47c23de3d43d2/confection-0.1.5-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<1.0.0,>=0.7.0 from https://files.pythonhosted.org/packages/bc/ba/d8f2c0151585519759135550574385dd7a223abbc6b6c06dab7ada565773/cloudpathlib-0.18.1-py3-none-any.whl.metadata\n",
      "  Downloading cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Obtaining dependency information for marisa-trie>=0.7.7 from https://files.pythonhosted.org/packages/61/28/b93cd14cd422be8fc091bd454dd48edbf0c2333111183db38c8e5a13e468/marisa_trie-1.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Downloading spacy-3.7.5-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/12.1 MB 4.2 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.3/12.1 MB 3.2 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/12.1 MB 2.8 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.5/12.1 MB 3.0 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.7/12.1 MB 3.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.9/12.1 MB 3.3 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.1/12.1 MB 3.6 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.3/12.1 MB 3.5 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.4/12.1 MB 3.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.5/12.1 MB 3.4 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.7/12.1 MB 3.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.9/12.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/12.1 MB 3.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.2/12.1 MB 3.5 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.5/12.1 MB 3.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.6/12.1 MB 3.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.7/12.1 MB 3.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.8/12.1 MB 3.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.0/12.1 MB 3.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.1/12.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.2/12.1 MB 3.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.3/12.1 MB 3.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.1 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.1 MB 3.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.5/12.1 MB 3.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.6/12.1 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.6/12.1 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.7/12.1 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.8/12.1 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.8/12.1 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.9/12.1 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 4.0/12.1 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 4.0/12.1 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 4.1/12.1 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 4.2/12.1 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.3/12.1 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.4/12.1 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.4/12.1 MB 2.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.5/12.1 MB 2.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.6/12.1 MB 2.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.7/12.1 MB 2.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.8/12.1 MB 2.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.9/12.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.9/12.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 5.0/12.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 5.1/12.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.3/12.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.4/12.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 5.5/12.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 5.6/12.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 5.7/12.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.8/12.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.9/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 6.0/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 6.1/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 6.2/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 6.3/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.4/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.5/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.6/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.7/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.8/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.9/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 7.0/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 7.1/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 7.2/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.3/12.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.5/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.6/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.7/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.8/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.9/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.0/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.1/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.2/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.3/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.5/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.7/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.8/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.9/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.0/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.1/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.3/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.4/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.5/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.6/12.1 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.9/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.0/12.1 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.1/12.1 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.2/12.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.3/12.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.4/12.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.6/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.6/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.8/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.9/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.1/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.2/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 182.0/182.0 kB 3.7 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.3/122.3 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "   ---------------------------------------- 0.0/479.7 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 163.8/479.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 276.5/479.7 kB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 440.3/479.7 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 479.7/479.7 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 5.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.3/1.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.2/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.3/50.3 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/6.6 MB 3.9 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.3/6.6 MB 2.9 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.4/6.6 MB 3.1 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.6/6.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.7/6.6 MB 2.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.8/6.6 MB 2.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.0/6.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.1/6.6 MB 2.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.3/6.6 MB 3.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.4/6.6 MB 3.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.5/6.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.7/6.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.8/6.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.0/6.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.2/6.6 MB 3.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.3/6.6 MB 3.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.5/6.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.7/6.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.9/6.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.0/6.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.1/6.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.3/6.6 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.4/6.6 MB 3.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.4/6.6 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.5/6.6 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.6/6.6 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.6/6.6 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.7/6.6 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.8/6.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.0/6.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.0/6.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.1/6.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.2/6.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.3/6.6 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.6 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.6 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.5/6.6 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.8/6.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.9/6.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.0/6.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.1/6.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.2/6.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.2/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.3/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.4/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.5/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.6/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.7/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.9/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.0/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.1/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.2/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.3/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.4/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.5/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.3/47.3 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/5.4 MB 2.2 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.2/5.4 MB 2.1 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.3/5.4 MB 2.2 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.4/5.4 MB 2.1 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.5/5.4 MB 2.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.6/5.4 MB 2.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.7/5.4 MB 2.3 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.8/5.4 MB 2.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.9/5.4 MB 2.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.0/5.4 MB 2.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.2/5.4 MB 2.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.3/5.4 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.4/5.4 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.5/5.4 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.7/5.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.8/5.4 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.0/5.4 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.1/5.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.2/5.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.5/5.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.6/5.4 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.7/5.4 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.8/5.4 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.9/5.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.1/5.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.2/5.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.4/5.4 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.6/5.4 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.9/5.4 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.0/5.4 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.1/5.4 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.4/5.4 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.6/5.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.8/5.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 4.9/5.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.0/5.4 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.3/5.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 2.5 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.6/152.6 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 preshed-3.0.9 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "pip install spacy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36701efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60834b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf907651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "902d0de7-a644-459d-8c4f-efb425c2d976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f28c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85d4873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch resumes from web links\n",
    "def fetch_resumes(links):\n",
    "    resumes = []\n",
    "    for link in links:\n",
    "        response = requests.get(link)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        resumes.append(soup.get_text())\n",
    "    return resumes\n",
    "\n",
    "# Example web links\n",
    "links = [\n",
    "        'https://drive.google.com/file/d/14akVZY0QKo04F3-Ere_vDnIDznfWHaSd/view?usp=drive_link',\n",
    "    'https://drive.google.com/file/d/1UMFACm2ivUE2MbPgJRfLs46yfOnuDkah/view?usp=drive_link',\n",
    "    'https://drive.google.com/file/d/1j_RgoBeTV5AQwxkN7t-pdp87UiKoPv64/view?usp=drive_link'\n",
    "    # Add more links as needed\n",
    "]\n",
    "\n",
    "# Fetch resumes\n",
    "resumes = fetch_resumes(links)\n",
    "\n",
    "# Save resumes to a file\n",
    "with open('resumes.txt', 'w') as file:\n",
    "    for resume in resumes:\n",
    "        file.write(resume + '\\n')\n",
    "\n",
    "# Load resumes into a dataframe\n",
    "df = pd.DataFrame(resumes, columns=['resume_text'])\n",
    "\n",
    "# Preprocess resumes to extract metadata (this is a simplified example)\n",
    "def extract_metadata(resume):\n",
    "    # Extract work experience, education, etc.\n",
    "    return {'work_experience': 'example', 'education': 'example'}\n",
    "\n",
    "df['metadata'] = df['resume_text'].apply(extract_metadata)\n",
    "\n",
    "# Save preprocessed resumes\n",
    "df.to_csv('preprocessed_resumes.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "086aaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize resumes\n",
    "tokens = tokenizer(df['resume_text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Create embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**tokens)\n",
    "\n",
    "# Save embeddings\n",
    "torch.save(embeddings, 'embeddings.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22077c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize resumes\n",
    "tokens = tokenizer(df['resume_text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Create embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**tokens)\n",
    "\n",
    "# Save embeddings\n",
    "torch.save(embeddings, 'embeddings.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80d905c0-0c2a-450e-88d3-e5df6373d1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Computer vision developer who has built models for object identification'\n",
      " 'Python developer with FastAPI experience'\n",
      " 'Python developer with experience scaling AWS infrastructure']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example resumes\n",
    "resumes = [\n",
    "    \"Python developer with experience scaling AWS infrastructure\",\n",
    "    \"Computer vision developer who has built models for object identification\",\n",
    "    \"Python developer with FastAPI experience\",\n",
    "    # Add more resumes as needed\n",
    "]\n",
    "\n",
    "# Tokenize resumes\n",
    "tokens = tokenizer(resumes, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Create embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    resume_embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "\n",
    "# Example query\n",
    "query = \"Python developer with experience scaling AWS infrastructure\"\n",
    "query_tokens = tokenizer(query, return_tensors='pt')\n",
    "\n",
    "# Create query embedding\n",
    "with torch.no_grad():\n",
    "    query_output = model(**query_tokens)\n",
    "    query_embedding = query_output.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarities = cosine_similarity(query_embedding.numpy(), resume_embeddings.numpy())\n",
    "similar_indices = np.argsort(similarities, axis=1).flatten()\n",
    "\n",
    "# Retrieve top results\n",
    "top_resumes = np.array(resumes)[similar_indices[:10]]  # Adjust number of results as needed\n",
    "\n",
    "# Display top results\n",
    "print(top_resumes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf59b420-2c72-4c68-b803-0298abc3a33f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# Load preprocessed resumes and embeddings\n",
    "df = pd.read_csv('preprocessed_resumes.csv')\n",
    "embeddings = torch.load('embeddings.pt')\n",
    "\n",
    "# Streamlit UI\n",
    "st.title('Resume Semantic Search')\n",
    "query = st.text_input('Enter your query:')\n",
    "if query:\n",
    "    query_tokens = tokenizer(query, return_tensors='pt')\n",
    "\n",
    "    # Create query embedding\n",
    "    with torch.no_grad():\n",
    "        query_output = model(**query_tokens)\n",
    "        query_embedding = query_output.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(query_embedding.numpy(), embeddings.numpy())\n",
    "    similar_indices = np.argsort(similarities, axis=1).flatten()\n",
    "\n",
    "    # Retrieve top results\n",
    "    top_resumes = df.iloc[similar_indices[:10]]  # Adjust number of results as needed\n",
    "\n",
    "    # Display top results\n",
    "    st.write(top_resumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07398f40-d503-4a51-b8c7-af35e5200de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e42bc-2bd4-4d6a-abf5-3ab1cd404e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e09d0a23-c3f2-40ce-9975-e87cfaf05755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pinecone-client streamlit pandas sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e50ce4f-87dc-421f-87d1-fcb632a1d8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca1cc61-66ba-45ce-8572-36b062f10fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3449e7a-9ba2-4d43-99c5-d2e4296fdb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall docx -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d087a00-8f31-4501-8172-f45ab5aeae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def load_resumes_from_folder(folder_path):\n",
    "    resumes = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf') or filename.endswith('.docx'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            resumes.append((filename, open(filepath, 'rb').read()))\n",
    "    return resumes\n",
    "\n",
    "def load_resumes_from_links(links):\n",
    "    resumes = []\n",
    "    for link in links:\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            resumes.append((link, response.content))\n",
    "    return resumes\n",
    "\n",
    "# Example usage\n",
    "#folder_path = 'path/to/resume/folder'\n",
    "#resumes = load_resumes_from_folder(folder_path)\n",
    "\n",
    "# Or from links\n",
    "links = [r'https://drive.google.com/drive/folders/1A0GPGrH7rLlAFNJ-RftYzLp28fJ0kKSU?usp=sharing']\n",
    "resumes = load_resumes_from_links(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5686c15-9116-4c7a-a7d1-505694a87ea3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'seek'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([para\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m para \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mparagraphs])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m parsed_resumes \u001b[38;5;241m=\u001b[39m [(name, parse_resume(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocx\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m name, file \u001b[38;5;129;01min\u001b[39;00m resumes]\n",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([para\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m para \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mparagraphs])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m parsed_resumes \u001b[38;5;241m=\u001b[39m [(name, \u001b[43mparse_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.pdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m name, file \u001b[38;5;129;01min\u001b[39;00m resumes]\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mparse_resume\u001b[1;34m(file, file_type)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([page\u001b[38;5;241m.\u001b[39mextract_text() \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf\u001b[38;5;241m.\u001b[39mpages])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocx\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([para\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m para \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mparagraphs])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\docx\\api.py:27\u001b[0m, in \u001b[0;36mDocument\u001b[1;34m(docx)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a |Document| object loaded from `docx`, where `docx` can be either a path\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03mto a ``.docx`` file (a string) or a file-like object.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03mIf `docx` is missing or ``None``, the built-in default document \"template\" is\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03mloaded.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m docx \u001b[38;5;241m=\u001b[39m _default_docx_path() \u001b[38;5;28;01mif\u001b[39;00m docx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m docx\n\u001b[1;32m---> 27\u001b[0m document_part \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocumentPart\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mPackage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmain_document_part)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m document_part\u001b[38;5;241m.\u001b[39mcontent_type \u001b[38;5;241m!=\u001b[39m CT\u001b[38;5;241m.\u001b[39mWML_DOCUMENT_MAIN:\n\u001b[0;32m     29\u001b[0m     tmpl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a Word file, content type is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\docx\\opc\\package.py:127\u001b[0m, in \u001b[0;36mOpcPackage.open\u001b[1;34m(cls, pkg_file)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pkg_file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m IO[\u001b[38;5;28mbytes\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OpcPackage:\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return an |OpcPackage| instance loaded with the contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m     pkg_reader \u001b[38;5;241m=\u001b[39m \u001b[43mPackageReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     package \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[0;32m    129\u001b[0m     Unmarshaller\u001b[38;5;241m.\u001b[39munmarshal(pkg_reader, package, PartFactory)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\docx\\opc\\pkgreader.py:22\u001b[0m, in \u001b[0;36mPackageReader.from_file\u001b[1;34m(pkg_file)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(pkg_file):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a |PackageReader| instance loaded with contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     phys_reader \u001b[38;5;241m=\u001b[39m \u001b[43mPhysPkgReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     content_types \u001b[38;5;241m=\u001b[39m _ContentTypeMap\u001b[38;5;241m.\u001b[39mfrom_xml(phys_reader\u001b[38;5;241m.\u001b[39mcontent_types_xml)\n\u001b[0;32m     24\u001b[0m     pkg_srels \u001b[38;5;241m=\u001b[39m PackageReader\u001b[38;5;241m.\u001b[39m_srels_for(phys_reader, PACKAGE_URI)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\docx\\opc\\phys_pkg.py:76\u001b[0m, in \u001b[0;36m_ZipPkgReader.__init__\u001b[1;34m(self, pkg_file)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pkg_file):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_ZipPkgReader, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zipf \u001b[38;5;241m=\u001b[39m \u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\zipfile.py:1271\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1271\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1273\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\zipfile.py:1334\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1332\u001b[0m fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1334\u001b[0m     endrec \u001b[38;5;241m=\u001b[39m \u001b[43m_EndRecData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\zipfile.py:266\u001b[0m, in \u001b[0;36m_EndRecData\u001b[1;34m(fpin)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return data from the \"End of Central Directory\" record, or None.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03mThe data is a list of the nine items in the ZIP \"End of central dir\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03mrecord followed by a tenth item, the file seek offset of this record.\"\"\"\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# Determine file size\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mfpin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    267\u001b[0m filesize \u001b[38;5;241m=\u001b[39m fpin\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# Check to see if this is ZIP file with no archive comment (the\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# \"end of central directory\" structure should be the last item in the\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# file if this is the case).\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'seek'"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import docx\n",
    "from docx import Document\n",
    "\n",
    "def parse_resume(file, file_type):\n",
    "    if file_type == 'pdf':\n",
    "        with pdfplumber.open(file) as pdf:\n",
    "            return ''.join([page.extract_text() for page in pdf.pages])\n",
    "    elif file_type == 'docx':\n",
    "        doc = Document(file)\n",
    "        return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Example usage\n",
    "parsed_resumes = [(name, parse_resume(file, 'pdf' if name.endswith('.pdf') else 'docx')) for name, file in resumes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa17b1d-7de5-485a-9fb8-501938247e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6e120-7b45-4d71-bbbf-92c9098e5ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ad614-b968-47e0-b06a-72c0204a14c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d8cbd3-1d5d-4da3-bd3e-76683055bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas faiss-cpu streamlit sentence-transformers requests pdfplumber tqdm -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a84ed457-4345-4289-abce-86a6c299f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def download_file_from_google_drive(file_id, destination):\n",
    "    URL = \"https://drive.google.com/drive/folders/1A0GPGrH7rLlAFNJ-RftYzLp28fJ0kKSU\"\n",
    "    session = requests.Session()\n",
    "    response = session.get(URL, params={'id': file_id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "    \n",
    "    if token:\n",
    "        params = {'id': file_id, 'confirm': token}\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "    \n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "def load_resumes_from_links(links):\n",
    "    resumes = []\n",
    "    for link in links:\n",
    "        file_id = link.split('/')[-2]\n",
    "        destination = f\"{file_id}.pdf\"\n",
    "        download_file_from_google_drive(file_id, destination)\n",
    "        resumes.append(destination)\n",
    "    return resumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32827711-20d9-4ad1-b762-c84590d6a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            text = ''\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or ''\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def embed_resumes(resumes):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = []\n",
    "    for resume in resumes:\n",
    "        text = extract_text_from_pdf(resume)\n",
    "        if text:  # Proceed only if text extraction was successful\n",
    "            embedding = model.encode(text)\n",
    "            embeddings.append(embedding)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c836415-939c-4ad3-921f-33e23a51c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def build_faiss_index(embeddings):\n",
    "    d = len(embeddings[0])\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(np.array(embeddings))\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f2a4f11-d924-4b81-9f4a-9d23af9dd373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_resumes(query, index, resumes):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    query_embedding = model.encode(query)\n",
    "    D, I = index.search(np.array([query_embedding]), k=10)\n",
    "    return [resumes[i] for i in I[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d93b1-069e-466e-b0d3-8863c882f3f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_results(query, index, resumes):\n",
    "    results = search_resumes(query, index, resumes)\n",
    "    print(f\"Query: {query}\")\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "# Example usage\n",
    "main()\n",
    "if __name__ == \"__main__\":\n",
    "    links = [\"https://drive.google.com/drive/folders/1A0GPGrH7rLlAFNJ-RftYzLp28fJ0kKSU\"]\n",
    "    resumes = load_resumes_from_links(links)\n",
    "    embeddings = embed_resumes(resumes)\n",
    "    if embeddings:  # Proceed only if embeddings were successfully created\n",
    "        index = build_faiss_index(embeddings)\n",
    "        \n",
    "        # Display results for a sample query\n",
    "        sample_query = \"Python developer with experience in machine learning\"\n",
    "        display_results(sample_query, index, resumes)\n",
    "    else:\n",
    "        print(\"No valid resumes found or embeddings were not created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa93a215-a3b1-497b-9f5d-c3c64c431657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "def main():\n",
    "    st.title(\"Resume Semantic Search\")\n",
    "    query = st.text_input(\"Enter your query:\")\n",
    "    if st.button(\"Search\"):\n",
    "        results = search_resumes(query, index, resumes)\n",
    "        for result in results:\n",
    "            st.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e313f5a1-c70c-4e02-8cba-3b097bcf1cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
